{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e2470379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow\n",
    "# !pip install fugashi\n",
    "# !pip install 'fugashi[unidic]'\n",
    "# !python -m unidic download\n",
    "# !pip install ntlk\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import fugashi\n",
    "import nltk\n",
    "import string\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "3652c0af-92b6-4a34-99f4-25a94b601445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "aa38e012-0d07-4f67-882a-8467cea6210d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras.preprocessing.text_dataset_from_directory'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[127], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_dataset_from_directory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JapaneseTokenizer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.preprocessing.text_dataset_from_directory'"
     ]
    }
   ],
   "source": [
    "keras.utils."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bac992-1207-4c3e-9d4f-07be13f38b7a",
   "metadata": {},
   "source": [
    "### Step 1 - Word Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48f2e6bd-5af6-4c42-806d-cbc5562824d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_japanese(text, tokenizer):\n",
    "    # Remove puncutation and tokenize text\n",
    "    cleaned_text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return [word.surface for word in tokenizer(cleaned_text)]\n",
    "\n",
    "def tokenize_english(text):\n",
    "    # Remove punctuation and tokenize text\n",
    "    cleaned_text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return [word for word in nltk.word_tokenize(cleaned_text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1767138",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_table('Data/split/train', index_col=None, header=None)\n",
    "data.rename(columns={0: 'English', \n",
    "                     1: 'Japanese'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d950fc5-2bf5-4091-94bf-c88bd28401a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tagged = pd.DataFrame(columns=data.columns)\n",
    "\n",
    "tagger = fugashi.Tagger()\n",
    "\n",
    "df_tagged['Japanese'] = data['Japanese'].apply(lambda text: tokenize_japanese(text, tagger))\n",
    "df_tagged['English']  = data['English'].apply(lambda text: tokenize_english(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0ff7bd28-422b-4cc7-863c-71ac73f05cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>Japanese</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[its, suliban]</td>\n",
       "      <td>[ス, リバン, 人, です]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[nothing, thrills, me, more, than, to, see, ki...</td>\n",
       "      <td>[生徒, が, お, 互い, の, 受精, じゃ, なく, て, 植物, の, 受粉, に,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[do, you, want, to, spend, all, night, at, the...</td>\n",
       "      <td>[この, 雨, の, 中, 一, 晩, 中, 墓地, に, いたい]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[our, ships, are, in, attack, position]</td>\n",
       "      <td>[船, は, もう, 攻撃, 発起, 位置, に, ある, 。]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[what, about, the, original, sam, huh]</td>\n",
       "      <td>[オリジナル, の, サム, が, 居る, ん, だ, ぞ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2694506</th>\n",
       "      <td>[what, good, would, that, have, done]</td>\n",
       "      <td>[知っ, て, 何, の, 意味, が, ある]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2694507</th>\n",
       "      <td>[when, ichinosukesan, rushed, over, to, ninosu...</td>\n",
       "      <td>[二, 之助, さん, に, 駆け寄っ, た, 一之助, さん, は]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2694508</th>\n",
       "      <td>[your, words, are, so, encouraging]</td>\n",
       "      <td>[そう, いっ, た, お, 言葉, を, 力, に, 。]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2694509</th>\n",
       "      <td>[goodbye, george]</td>\n",
       "      <td>[切る, ぞ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2694510</th>\n",
       "      <td>[do, you, want, to, hear, it]</td>\n",
       "      <td>[聞き, たく, なけれ, ば, 言わ, ない, けれど, 。]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2636596 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   English  \\\n",
       "0                                           [its, suliban]   \n",
       "1        [nothing, thrills, me, more, than, to, see, ki...   \n",
       "2        [do, you, want, to, spend, all, night, at, the...   \n",
       "3                  [our, ships, are, in, attack, position]   \n",
       "4                   [what, about, the, original, sam, huh]   \n",
       "...                                                    ...   \n",
       "2694506              [what, good, would, that, have, done]   \n",
       "2694507  [when, ichinosukesan, rushed, over, to, ninosu...   \n",
       "2694508                [your, words, are, so, encouraging]   \n",
       "2694509                                  [goodbye, george]   \n",
       "2694510                      [do, you, want, to, hear, it]   \n",
       "\n",
       "                                                  Japanese  \n",
       "0                                          [ス, リバン, 人, です]  \n",
       "1        [生徒, が, お, 互い, の, 受精, じゃ, なく, て, 植物, の, 受粉, に,...  \n",
       "2                       [この, 雨, の, 中, 一, 晩, 中, 墓地, に, いたい]  \n",
       "3                         [船, は, もう, 攻撃, 発起, 位置, に, ある, 。]  \n",
       "4                           [オリジナル, の, サム, が, 居る, ん, だ, ぞ]  \n",
       "...                                                    ...  \n",
       "2694506                           [知っ, て, 何, の, 意味, が, ある]  \n",
       "2694507                [二, 之助, さん, に, 駆け寄っ, た, 一之助, さん, は]  \n",
       "2694508                     [そう, いっ, た, お, 言葉, を, 力, に, 。]  \n",
       "2694509                                            [切る, ぞ]  \n",
       "2694510                   [聞き, たく, なけれ, ば, 言わ, ない, けれど, 。]  \n",
       "\n",
       "[2636596 rows x 2 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abcd674-5450-467b-afa3-70e860258c79",
   "metadata": {},
   "source": [
    "### Step 2 - Create Word Mapping Dictionaries (for Japanese and English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "391bd9df-9c99-4d80-9a00-38982c586c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_mapping(data):\n",
    "    # Create a dictionary to map words to an index (words2idx, and idx2words)\n",
    "    \n",
    "    words2idx     = {}\n",
    "    idx2words     = {}\n",
    "\n",
    "    words2idx['PAD'] = 0\n",
    "    idx2words[0]     = '<PAD>'\n",
    "    \n",
    "    words2idx['SOS'] = 1\n",
    "    idx2words[0]     = '<SOS>'\n",
    "    \n",
    "    words2idx['UNK']  = 2\n",
    "    idx2words[1]  = '<UNK>'\n",
    "    \n",
    "    counter  = len(words2idx)\n",
    "    \n",
    "    for text in data:\n",
    "        unique_words = set(text)\n",
    "        for word in unique_words:\n",
    "            if word in words2idx.keys():\n",
    "                continue\n",
    "            else:\n",
    "                words2idx[word]    = counter\n",
    "                idx2words[counter] = word\n",
    "                counter           += 1\n",
    "    \n",
    "    words2idx['EOS']        = counter\n",
    "    idx2words[counter]      = '<EOS>'\n",
    "\n",
    "    return words2idx, idx2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f36b18c-74bd-4725-b8ac-d88cc7aaa6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "words2idx_en, idx2words_en = generate_word_mapping(df_tagged['English'].tolist())\n",
    "words2idx_jp, idx2words_jp = generate_word_mapping(df_tagged['Japanese'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2247a6d-d866-4281-8411-c9e70c0372ef",
   "metadata": {},
   "source": [
    "### Step 3 - Convert to TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d34f9d85-d2bb-4279-880b-6f13056ecac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sos, eos = 'SOS', 'EOS'\n",
    "def to_tf_string_input(text):\n",
    "    return f'<{SOS}> ' + ' '.join(text)\n",
    "\n",
    "def to_tf_string_output(text):\n",
    "    return ' '.join(text) + f' <{EOS}>'\n",
    "\n",
    "def convert_to_tensor(text, dictionary, input=True):\n",
    "    if input:\n",
    "        tf_string = to_tf_string_input(text)\n",
    "        \n",
    "    else:\n",
    "        tf_string = to_tf_string_output(text)\n",
    "    return tf.convert_to_tensor([dictionary[x.decode('utf-8')] for x in tf.strings.split(tf_string).numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "78e7b3f9-eed7-4a1c-b38d-8466f6e7f0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Restrict text length to 20\n",
    "idx       = df_tagged[(df_tagged['English'].apply(lambda x: len(x)) <= 20) & (df_tagged['Japanese'].apply(lambda x: len(x)) <= 20)].index\n",
    "df_tagged = df_tagged.loc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "572f436d-45d1-48b1-a695-4093bce8fe9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build Model\n",
    "# Convert Text to Numbers\n",
    "eng_data_input  = df_tagged['English'][:50000].apply(lambda x: convert_to_tensor(x, words2idx_en))\n",
    "eng_data_output = df_tagged['English'][:50000].apply(lambda x: convert_to_tensor(x, words2idx_en, input=False))\n",
    "jap_data        = df_tagged['Japanese'][:50000].apply(lambda x: convert_to_tensor(x, words2idx_jp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7de626-29a8-4441-8053-2c5184200aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad the input sequences\n",
    "encoder_inputs = pad_sequences(input_sequences, maxlen=input_max_len, padding='post')\n",
    "print(\"encoder_inputs.shape:\", encoder_inputs.shape)\n",
    "print(\"encoder_inputs[0]:\", encoder_inputs[0])\n",
    "# pad the decoder input sequences\n",
    "decoder_inputs = pad_sequences(target_sequences_inputs, maxlen=target_max_len, padding='post')\n",
    "print(\"decoder_inputs[0]:\", decoder_inputs[0])\n",
    "print(\"decoder_inputs.shape:\", decoder_inputs.shape)\n",
    "# pad the target output sequences\n",
    "decoder_targets = pad_sequences(target_sequences, maxlen=target_max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "21315da6-5979-4322-99a6-fc751bdd5621",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = tf.convert_to_tensor(tf.keras.utils.pad_sequences(eng_data, padding='post'))\n",
    "# decoder_inputs = tf.convert_to_tensor(tf.keras.utils.pad_sequences(eng_data, padding='post'))\n",
    "\n",
    "decoder_targets = tf.convert_to_tensor(tf.keras.utils.pad_sequences(jap_data, padding='post'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c889f43d-d9a4-4a7e-abd0-722e7fa60715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(50000, 22), dtype=int32, numpy=\n",
       "array([[     1,      3,      4, ...,      0,      0,      0],\n",
       "       [     1,     13,     11, ...,     20, 173111,      0],\n",
       "       [     1,     24,     29, ...,      0,      0,      0],\n",
       "       ...,\n",
       "       [     1,   2592,    121, ...,      0,      0,      0],\n",
       "       [     1,   1205,     20, ...,      0,      0,      0],\n",
       "       [     1,    260,     14, ...,      0,      0,      0]], dtype=int32)>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcd770a-acce-4ffd-9148-31bbbfe8341e",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "df81caa0-1b84-470c-ada2-be8d262f3bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 50000\n",
    "BATCH_SIZE = 64  # Batch size for training.\n",
    "EPOCHS = 10  # Number of epochs to train for.\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 1024\n",
    "\n",
    "ATTENTION_FUNC='general'\n",
    "UNITS = 64\n",
    "VOCAB_SIZE = len(words2idx_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "09ff1dde-0b08-4606-a69f-e87adc6caff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # Define the embedding layer\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        # Define the RNN layer, LSTM\n",
    "        self.lstm = tf.keras.layers.LSTM(\n",
    "            hidden_dim, return_sequences=True, return_state=True)\n",
    "\n",
    "    def call(self, input_sequence, states):\n",
    "        # Embed the input\n",
    "        embed = self.embedding(input_sequence)\n",
    "        # Call the LSTM unit\n",
    "        output, state_h, state_c = self.lstm(embed, initial_state=states)\n",
    "\n",
    "        return output, state_h, state_c\n",
    "\n",
    "    def init_states(self, batch_size):\n",
    "        # Return a all 0s initial states\n",
    "        return (tf.zeros([batch_size, self.hidden_dim]),\n",
    "                tf.zeros([batch_size, self.hidden_dim]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8599dcc-9f2e-4dde-82ec-ec9e80c84d9c",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "dbc97224-5c4d-4b84-9762-198eb8942f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # Define the embedding layer\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        # Define the RNN layer, LSTM\n",
    "        self.lstm = tf.keras.layers.LSTM(\n",
    "            hidden_dim, return_sequences=True, return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, input_sequence, state):\n",
    "        # Embed the input\n",
    "        embed = self.embedding(input_sequence)\n",
    "        # Call the LSTM unit\n",
    "        lstm_out, state_h, state_c = self.lstm(embed, state)\n",
    "        # Dense layer to predict output token\n",
    "        logits = self.dense(lstm_out)\n",
    "\n",
    "        return logits, state_h, state_c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "21a92210-f848-49a8-ad25-04d76dcf63b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 6, 1024)\n",
      "(1, 8, 173112)\n"
     ]
    }
   ],
   "source": [
    "#Set the length of the input and output vocabulary\n",
    "num_words_inputs = len(words2idx_en) + 1\n",
    "num_words_output = len(words2idx_jp) + 1\n",
    "#Create the encoder\n",
    "encoder = Encoder(num_words_inputs, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "# Get the initial states\n",
    "initial_state = encoder.init_states(1)\n",
    "# Call the encoder for testing\n",
    "test_encoder_output = encoder(tf.constant(\n",
    "    [[1, 23, 4, 5, 0, 0]]), initial_state)\n",
    "print(test_encoder_output[0].shape)\n",
    "# Create the decoder\n",
    "decoder = Decoder(num_words_output, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "# Get the initial states\n",
    "de_initial_state = test_encoder_output[1:]\n",
    "# Call the decoder for testing\n",
    "test_decoder_output = decoder(tf.constant(\n",
    "    [[1, 3, 5, 7, 9, 0, 0, 0]]), de_initial_state)\n",
    "print(test_decoder_output[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dcdef6-433a-41af-9004-1e1c318f4004",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5e7d1e42-eb56-4c7d-bd88-1d85f348680a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(targets, logits):\n",
    "    crossentropy = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True)\n",
    "    # Mask padding values, they do not have to compute for loss\n",
    "    mask = tf.math.logical_not(tf.math.equal(targets, 0))\n",
    "    mask = tf.cast(mask, dtype=tf.int64)\n",
    "    # Calculate the loss value\n",
    "    loss = crossentropy(targets, logits, sample_weight=mask)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    # y_pred shape is batch_size, seq length, vocab size\n",
    "    # y_true shape is batch_size, seq length\n",
    "    pred_values = K.cast(K.argmax(y_pred, axis=-1), dtype='int32')\n",
    "    correct = K.cast(K.equal(y_true, pred_values), dtype='float32')\n",
    "\n",
    "    # 0 is padding, don't include those\n",
    "    mask = K.cast(K.greater(y_true, 0), dtype='float32')\n",
    "    n_correct = K.sum(mask * correct)\n",
    "    n_total = K.sum(mask)\n",
    "  \n",
    "    return n_correct / n_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fbad6a68-1839-4b8e-b319-efe5126e8bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the @tf.function decorator to take advance of static graph computation\n",
    "@tf.function\n",
    "def train_step(input_seq, target_seq_in, target_seq_out, en_initial_states, optimizer):\n",
    "    ''' A training step, train a batch of the data and return the loss value reached\n",
    "        Input:\n",
    "        - input_seq: array of integers, shape [batch_size, max_seq_len, embedding dim].\n",
    "            the input sequence\n",
    "        - target_seq_out: array of integers, shape [batch_size, max_seq_len, embedding dim].\n",
    "            the target seq, our target sequence\n",
    "        - target_seq_in: array of integers, shape [batch_size, max_seq_len, embedding dim].\n",
    "            the input sequence to the decoder, we use Teacher Forcing\n",
    "        - en_initial_states: tuple of arrays of shape [batch_size, hidden_dim].\n",
    "            the initial state of the encoder\n",
    "        - optimizer: a tf.keras.optimizers.\n",
    "        Output:\n",
    "        - loss: loss value\n",
    "        \n",
    "    '''\n",
    "    # Network’s computations need to be put under tf.GradientTape() to keep track of gradients\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Get the encoder outputs\n",
    "        en_outputs = encoder(input_seq, en_initial_states)\n",
    "        # Set the encoder and decoder states\n",
    "        en_states = en_outputs[1:]\n",
    "        de_states = en_states\n",
    "        # Get the encoder outputs\n",
    "        de_outputs = decoder(target_seq_in, de_states)\n",
    "        # Take the actual output\n",
    "        logits = de_outputs[0]\n",
    "        # Calculate the loss function\n",
    "        loss = loss_func(target_seq_out, logits)\n",
    "        acc = accuracy_fn(target_seq_out, logits)\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    # Calculate the gradients for the variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    # Apply the gradients and update the optimizer\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return loss, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f9f508b6-7b34-4eab-9fc8-8c90e214db17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the main train function\n",
    "def main_train(encoder, decoder, dataset, n_epochs, batch_size, optimizer, checkpoint, checkpoint_prefix):\n",
    "    \n",
    "    losses = []\n",
    "    accuracies = []\n",
    "\n",
    "    for e in range(n_epochs):\n",
    "        # Get the initial time\n",
    "        start = time.time()\n",
    "        # Get the initial state for the encoder\n",
    "        en_initial_states = encoder.init_states(batch_size)\n",
    "        # For every batch data\n",
    "        for batch, (input_seq, target_seq_in, target_seq_out) in enumerate(dataset.take(-1)):\n",
    "            # Train and get the loss value \n",
    "            loss, accuracy = train_step(input_seq, target_seq_in, target_seq_out, en_initial_states, optimizer)\n",
    "        \n",
    "            if batch % 100 == 0:\n",
    "                # Store the loss and accuracy values\n",
    "                losses.append(loss)\n",
    "                accuracies.append(accuracy)\n",
    "                print('Epoch {} Batch {} Loss {:.4f} Acc:{:.4f}'.format(e + 1, batch, loss.numpy(), accuracy.numpy()))\n",
    "                \n",
    "        # saving (checkpoint) the model every 2 epochs\n",
    "        if (e + 1) % 2 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    \n",
    "        print('Time taken for 1 epoch {:.4f} sec\\n'.format(time.time() - start))\n",
    "        \n",
    "    return losses, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5070125a-083e-4088-af96-54936d3cd5f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[97], line 10\u001b[0m\n\u001b[1;32m      5\u001b[0m checkpoint_prefix \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(checkpoint_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mCheckpoint(optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[1;32m      7\u001b[0m                                  encoder\u001b[38;5;241m=\u001b[39mencoder,\n\u001b[1;32m      8\u001b[0m                                  decoder\u001b[38;5;241m=\u001b[39mdecoder)\n\u001b[0;32m---> 10\u001b[0m losses, accuracies \u001b[38;5;241m=\u001b[39m main_train(encoder, decoder, dataset, EPOCHS, BATCH_SIZE, optimizer, checkpoint, checkpoint_prefix)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Create an Adam optimizer and clips gradients by norm\n",
    "optimizer = tf.keras.optimizers.Adam(clipnorm=5.0)\n",
    "# Create a checkpoint object to save the model\n",
    "checkpoint_dir = './training_ckpt_seq2seq'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)\n",
    "\n",
    "losses, accuracies = main_train(encoder, decoder, dataset, EPOCHS, BATCH_SIZE, optimizer, checkpoint, checkpoint_prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfd59d9-4d25-4c50-956b-9a3d3bdd41cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "65010ec1-b269-48b9-b1e8-103a60162b22",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Encoder.__init__() missing 1 required positional argument: 'hidden_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Encode the input sequence.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m encoder \u001b[38;5;241m=\u001b[39m Encoder(UNITS, VOCAB_SIZE)\n\u001b[1;32m      3\u001b[0m ex_context \u001b[38;5;241m=\u001b[39m encoder(padded_en)\n",
      "\u001b[0;31mTypeError\u001b[0m: Encoder.__init__() missing 1 required positional argument: 'hidden_dim'"
     ]
    }
   ],
   "source": [
    "# Encode the input sequence.\n",
    "encoder = Encoder(UNITS, VOCAB_SIZE)\n",
    "ex_context = encoder(padded_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774fe24a-8baa-4889-989c-468dc58bdbaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
