{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8f1da782-436d-4614-80cd-769fed7213f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import MeCab\n",
    "import numpy as np\n",
    "import io\n",
    "import spacy\n",
    "import string\n",
    "import os\n",
    "#pip install mecab-python3\n",
    "#pip install unidic-lite\n",
    "#pip install spacy\n",
    "\n",
    "# Define Japanese Tagger - Mecab\n",
    "ja_tagger = MeCab.Tagger(\"-Owakati\") # default dictionary for parsing japanese\n",
    "\n",
    "# Define English Tagger - spaCy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "en_tagger = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner']) # Use en_core_web_sm to parse English"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4db0910-7649-4165-b728-1d1ef205aee8",
   "metadata": {},
   "source": [
    "### preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "12998430-5b45-4e77-902f-e898ecf360e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ja_en_data/split/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m     os\u001b[38;5;241m.\u001b[39mremove(fpath)\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \n\u001b[0;32m---> 40\u001b[0m \u001b[43mread_and_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mja_en_data/split/train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m read_and_process_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mja_en_data/split/train\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[105], line 19\u001b[0m, in \u001b[0;36mread_and_process_data\u001b[0;34m(fpath)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Reads the Japanese and English corpus and creates\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m2 separate files (.ja and .en) for each train, dev and test dataset\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03min the same data folder\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m@param fpath (str): filepath to read data\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# reads data\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m d \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# split on '\\n' to get a list where each item is a Eng2Jap example\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# second split on '\\t' to split each item into 2; first subitem is Eng translation, second subitem is Jap translation\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ja_en_data/split/train'"
     ]
    }
   ],
   "source": [
    "def write_file(fpath, content):\n",
    "    \"\"\" Writes content to a file in fpath\n",
    "    @param fpath (str): filepath to read data\n",
    "    @param content (str): file content\n",
    "    \"\"\"\n",
    "    f = open(fpath, 'w')\n",
    "    f.write(content)\n",
    "    f.close()\n",
    "    return\n",
    "\n",
    "def read_and_process_data(fpath):\n",
    "    \"\"\" Reads the Japanese and English corpus and creates\n",
    "    2 separate files (.ja and .en) for each train, dev and test dataset\n",
    "    in the same data folder\n",
    "    @param fpath (str): filepath to read data\n",
    "    \"\"\"\n",
    "\n",
    "    # reads data\n",
    "    f = io.open(fpath, mode=\"r\", encoding=\"utf-8\")\n",
    "    d = f.read()\n",
    "    \n",
    "    # split on '\\n' to get a list where each item is a Eng2Jap example\n",
    "    # second split on '\\t' to split each item into 2; first subitem is Eng translation, second subitem is Jap translation\n",
    "    examples = [e.split('\\t') for e in d.split('\\n')]\n",
    "\n",
    "    # creates 2 variables to hold english and japanese example\n",
    "    # only look for examples where there is a 1 to 1 map (so length == 2)\n",
    "    en_ex, ja_ex = [e[0] for e in examples if len(e) == 2], [e[1] for e in examples if len(e) == 2] \n",
    "\n",
    "    # join on '\\n' to create the file\n",
    "    opath_en = f'{fpath}.en'\n",
    "    opath_ja = f'{fpath}.ja'\n",
    "    \n",
    "    write_file(opath_en, '\\n'.join(en_ex))\n",
    "    write_file(opath_ja, '\\n'.join(ja_ex))\n",
    "\n",
    "    os.remove(fpath)\n",
    "    return \n",
    "\n",
    "read_and_process_data('ja_en_data/split/train')\n",
    "read_and_process_data('ja_en_data/split/dev')\n",
    "read_and_process_data('ja_en_data/split/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443c93e1-ec61-4483-864a-ded8b09f0805",
   "metadata": {},
   "source": [
    "### utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "84dc6c2e-9d80-49f2-9f19-4edab2c93d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3008d149-6f5f-468a-a21d-feb3b6b52d15",
   "metadata": {},
   "source": [
    "### vocab.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2fb23f87-6c33-4168-a678-efc6e3f597d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10': 4,\n",
       " '100000': 5,\n",
       " '11': 6,\n",
       " '12': 7,\n",
       " '120': 8,\n",
       " '1200': 9,\n",
       " '15': 10,\n",
       " '150': 11,\n",
       " '1500': 12,\n",
       " '17': 13,\n",
       " '18': 14,\n",
       " '180': 15,\n",
       " '1970s': 16,\n",
       " '1995': 17,\n",
       " '1996': 18,\n",
       " '2': 19,\n",
       " '20': 20,\n",
       " '200': 21,\n",
       " '25': 22,\n",
       " '26': 23,\n",
       " '30': 24,\n",
       " '35': 25,\n",
       " '38': 26,\n",
       " '45': 27,\n",
       " '450': 28,\n",
       " '46': 29,\n",
       " '49th': 30,\n",
       " '50': 31,\n",
       " '500th': 32,\n",
       " '55': 33,\n",
       " '5550167': 34,\n",
       " '58': 35,\n",
       " '6': 36,\n",
       " '65': 37,\n",
       " '74': 38,\n",
       " '7cfi06x': 39,\n",
       " '81': 40,\n",
       " '8track': 41,\n",
       " '9': 42,\n",
       " 'a': 43,\n",
       " 'aaron': 44,\n",
       " 'abaddon': 45,\n",
       " 'abandoned': 46,\n",
       " 'abby': 47,\n",
       " 'ability': 48,\n",
       " 'abkhazia': 49,\n",
       " 'able': 50,\n",
       " 'about': 51,\n",
       " 'above': 52,\n",
       " 'absolutely': 53,\n",
       " 'abu': 54,\n",
       " 'accept': 55,\n",
       " 'acceptable': 56,\n",
       " 'accepting': 57,\n",
       " 'access': 58,\n",
       " 'according': 59,\n",
       " 'accounted': 60,\n",
       " 'accumulate': 61,\n",
       " 'accustomed': 62,\n",
       " 'achievement': 63,\n",
       " 'acquaintance': 64,\n",
       " 'acre': 65,\n",
       " 'across': 66,\n",
       " 'act': 67,\n",
       " 'acting': 68,\n",
       " 'action': 69,\n",
       " 'actionable': 70,\n",
       " 'active': 71,\n",
       " 'actual': 72,\n",
       " 'actually': 73,\n",
       " 'adam': 74,\n",
       " 'adapt': 75,\n",
       " 'address': 76,\n",
       " 'adjust': 77,\n",
       " 'adler': 78,\n",
       " 'administration': 79,\n",
       " 'admit': 80,\n",
       " 'admitting': 81,\n",
       " 'adorable': 82,\n",
       " 'adrian': 83,\n",
       " 'advanced': 84,\n",
       " 'advertisement': 85,\n",
       " 'advised': 86,\n",
       " 'advisor': 87,\n",
       " 'affect': 88,\n",
       " 'afghanistan': 89,\n",
       " 'afraid': 90,\n",
       " 'africa': 91,\n",
       " 'after': 92,\n",
       " 'again': 93,\n",
       " 'against': 94,\n",
       " 'age': 95,\n",
       " 'agent': 96,\n",
       " 'agerelated': 97,\n",
       " 'ago': 98,\n",
       " 'agonizing': 99,\n",
       " 'agree': 100,\n",
       " 'agreed': 101,\n",
       " 'ah': 102,\n",
       " 'ahead': 103,\n",
       " 'ahsoka': 104,\n",
       " 'ai': 105,\n",
       " 'aight': 106,\n",
       " 'aimee': 107,\n",
       " 'air': 108,\n",
       " 'alan': 109,\n",
       " 'alas': 110,\n",
       " 'alcohol': 111,\n",
       " 'alderman': 112,\n",
       " 'alec': 113,\n",
       " 'alibi': 114,\n",
       " 'alien': 115,\n",
       " 'alive': 116,\n",
       " 'all': 117,\n",
       " 'allow': 118,\n",
       " 'allowed': 119,\n",
       " 'alma': 120,\n",
       " 'almost': 121,\n",
       " 'alone': 122,\n",
       " 'along': 123,\n",
       " 'already': 124,\n",
       " 'alright': 125,\n",
       " 'also': 126,\n",
       " 'altitude': 127,\n",
       " 'always': 128,\n",
       " 'am': 129,\n",
       " 'amazing': 130,\n",
       " 'ambo': 131,\n",
       " 'ambulance': 132,\n",
       " 'america': 133,\n",
       " 'american': 134,\n",
       " 'americans': 135,\n",
       " 'ammo': 136,\n",
       " 'among': 137,\n",
       " 'amore': 138,\n",
       " 'amount': 139,\n",
       " 'amplifying': 140,\n",
       " 'amurosan': 141,\n",
       " 'an': 142,\n",
       " 'analysis': 143,\n",
       " 'analysts': 144,\n",
       " 'analyzed': 145,\n",
       " 'ancient': 146,\n",
       " 'and': 147,\n",
       " 'anderson': 148,\n",
       " 'andorian': 149,\n",
       " 'angel': 150,\n",
       " 'angry': 151,\n",
       " 'animal': 152,\n",
       " 'animals': 153,\n",
       " 'anna': 154,\n",
       " 'anne': 155,\n",
       " 'anonymous': 156,\n",
       " 'another': 157,\n",
       " 'answer': 158,\n",
       " 'answers': 159,\n",
       " 'any': 160,\n",
       " 'anybody': 161,\n",
       " 'anymore': 162,\n",
       " 'anyone': 163,\n",
       " 'anything': 164,\n",
       " 'anyway': 165,\n",
       " 'anywhere': 166,\n",
       " 'aoi': 167,\n",
       " 'apologies': 168,\n",
       " 'apologize': 169,\n",
       " 'apology': 170,\n",
       " 'apparently': 171,\n",
       " 'appear': 172,\n",
       " 'appears': 173,\n",
       " 'appetite': 174,\n",
       " 'application': 175,\n",
       " 'applied': 176,\n",
       " 'appreciate': 177,\n",
       " 'appropriate': 178,\n",
       " 'approval': 179,\n",
       " 'april': 180,\n",
       " 'arabic': 181,\n",
       " 'archer': 182,\n",
       " 'arches': 183,\n",
       " 'are': 184,\n",
       " 'area': 185,\n",
       " 'argument': 186,\n",
       " 'arm': 187,\n",
       " 'arms': 188,\n",
       " 'army': 189,\n",
       " 'around': 190,\n",
       " 'arrangements': 191,\n",
       " 'arrays': 192,\n",
       " 'arrive': 193,\n",
       " 'arrived': 194,\n",
       " 'arrow': 195,\n",
       " 'art': 196,\n",
       " 'article': 197,\n",
       " 'artificial': 198,\n",
       " 'as': 199,\n",
       " 'asamoto': 200,\n",
       " 'ash': 201,\n",
       " 'ashamed': 202,\n",
       " 'ashes': 203,\n",
       " 'ask': 204,\n",
       " 'asked': 205,\n",
       " 'asleep': 206,\n",
       " 'ass': 207,\n",
       " 'assassins': 208,\n",
       " 'assault': 209,\n",
       " 'asshole': 210,\n",
       " 'assignment': 211,\n",
       " 'assumption': 212,\n",
       " 'at': 213,\n",
       " 'ate': 214,\n",
       " 'atmosphere': 215,\n",
       " 'atsushikun': 216,\n",
       " 'attaboy': 217,\n",
       " 'attack': 218,\n",
       " 'attacked': 219,\n",
       " 'attempted': 220,\n",
       " 'attend': 221,\n",
       " 'attendants': 222,\n",
       " 'attention': 223,\n",
       " 'audacious': 224,\n",
       " 'audience': 225,\n",
       " 'aunt': 226,\n",
       " 'aurelia': 227,\n",
       " 'automatically': 228,\n",
       " 'avenue': 229,\n",
       " 'average': 230,\n",
       " 'avoidance': 231,\n",
       " 'aw': 232,\n",
       " 'awabi': 233,\n",
       " 'aware': 234,\n",
       " 'away': 235,\n",
       " 'awful': 236,\n",
       " 'axe': 237,\n",
       " 'ayukawa': 238,\n",
       " 'baby': 239,\n",
       " 'back': 240,\n",
       " 'bacon': 241,\n",
       " 'bad': 242,\n",
       " 'bag': 243,\n",
       " 'bags': 244,\n",
       " 'bagura': 245,\n",
       " 'bailey': 246,\n",
       " 'balanced': 247,\n",
       " 'bali': 248,\n",
       " 'bank': 249,\n",
       " 'barry': 250,\n",
       " 'basically': 251,\n",
       " 'bates': 252,\n",
       " 'battery': 253,\n",
       " 'battle': 254,\n",
       " 'bay': 255,\n",
       " 'be': 256,\n",
       " 'bead': 257,\n",
       " 'bear': 258,\n",
       " 'bearer': 259,\n",
       " 'beast': 260,\n",
       " 'beastie': 261,\n",
       " 'beat': 262,\n",
       " 'beautiful': 263,\n",
       " 'became': 264,\n",
       " 'becasue': 265,\n",
       " 'because': 266,\n",
       " 'become': 267,\n",
       " 'bedside': 268,\n",
       " 'bee': 269,\n",
       " 'been': 270,\n",
       " 'beep': 271,\n",
       " 'before': 272,\n",
       " 'begged': 273,\n",
       " 'begin': 274,\n",
       " 'beginning': 275,\n",
       " 'begins': 276,\n",
       " 'behavior': 277,\n",
       " 'behind': 278,\n",
       " 'being': 279,\n",
       " 'believe': 280,\n",
       " 'bell': 281,\n",
       " 'belong': 282,\n",
       " 'belongs': 283,\n",
       " 'below': 284,\n",
       " 'belt': 285,\n",
       " 'benchmark': 286,\n",
       " 'bend': 287,\n",
       " 'best': 288,\n",
       " 'bet': 289,\n",
       " 'betacarotene': 290,\n",
       " 'betray': 291,\n",
       " 'better': 292,\n",
       " 'between': 293,\n",
       " 'big': 294,\n",
       " 'bigdog': 295,\n",
       " 'bigg': 296,\n",
       " 'bigger': 297,\n",
       " 'biggest': 298,\n",
       " 'bilingual': 299,\n",
       " 'bill': 300,\n",
       " 'bills': 301,\n",
       " 'biochemically': 302,\n",
       " 'biology': 303,\n",
       " 'bird': 304,\n",
       " 'birthday': 305,\n",
       " 'bit': 306,\n",
       " 'biting': 307,\n",
       " 'black': 308,\n",
       " 'blade': 309,\n",
       " 'blasphemy': 310,\n",
       " 'blast': 311,\n",
       " 'blind': 312,\n",
       " 'blindfold': 313,\n",
       " 'blood': 314,\n",
       " 'blow': 315,\n",
       " 'blue': 316,\n",
       " 'blunt': 317,\n",
       " 'board': 318,\n",
       " 'boat': 319,\n",
       " 'bobby': 320,\n",
       " 'boca': 321,\n",
       " 'bodies': 322,\n",
       " 'body': 323,\n",
       " 'boiled': 324,\n",
       " 'bomb': 325,\n",
       " 'bone': 326,\n",
       " 'bones': 327,\n",
       " 'bonks': 328,\n",
       " 'book': 329,\n",
       " 'boom': 330,\n",
       " 'borrow': 331,\n",
       " 'borrowed': 332,\n",
       " 'boss': 333,\n",
       " 'boston': 334,\n",
       " 'both': 335,\n",
       " 'bottle': 336,\n",
       " 'bought': 337,\n",
       " 'bounds': 338,\n",
       " 'bout': 339,\n",
       " 'boxes': 340,\n",
       " 'boy': 341,\n",
       " 'braddock': 342,\n",
       " 'brains': 343,\n",
       " 'brazil': 344,\n",
       " 'break': 345,\n",
       " 'breakfast': 346,\n",
       " 'breath': 347,\n",
       " 'breathe': 348,\n",
       " 'brent': 349,\n",
       " 'bright': 350,\n",
       " 'brilliant': 351,\n",
       " 'bring': 352,\n",
       " 'bringing': 353,\n",
       " 'brings': 354,\n",
       " 'brody': 355,\n",
       " 'broke': 356,\n",
       " 'broken': 357,\n",
       " 'bronze': 358,\n",
       " 'brother': 359,\n",
       " 'brothers': 360,\n",
       " 'brought': 361,\n",
       " 'broyles': 362,\n",
       " 'brush': 363,\n",
       " 'bryant': 364,\n",
       " 'bth': 365,\n",
       " 'bubble': 366,\n",
       " 'bucks': 367,\n",
       " 'budget': 368,\n",
       " 'building': 369,\n",
       " 'built': 370,\n",
       " 'bulb': 371,\n",
       " 'bullshit': 372,\n",
       " 'bunch': 373,\n",
       " 'bundle': 374,\n",
       " 'bunny': 375,\n",
       " 'burn': 376,\n",
       " 'burning': 377,\n",
       " 'bushes': 378,\n",
       " 'business': 379,\n",
       " 'businessmen': 380,\n",
       " 'busy': 381,\n",
       " 'but': 382,\n",
       " 'button': 383,\n",
       " 'buy': 384,\n",
       " 'buying': 385,\n",
       " 'by': 386,\n",
       " 'bye': 387,\n",
       " 'byte': 388,\n",
       " 'c': 389,\n",
       " 'ca': 390,\n",
       " 'cabinet': 391,\n",
       " 'cafe': 392,\n",
       " 'cage': 393,\n",
       " 'cairo': 394,\n",
       " 'california': 395,\n",
       " 'call': 396,\n",
       " 'called': 397,\n",
       " 'calling': 398,\n",
       " 'calls': 399,\n",
       " 'calm': 400,\n",
       " 'caltech': 401,\n",
       " 'came': 402,\n",
       " 'camera': 403,\n",
       " 'cameras': 404,\n",
       " 'camouflage': 405,\n",
       " 'can': 406,\n",
       " 'canal': 407,\n",
       " 'cancel': 408,\n",
       " 'cancer': 409,\n",
       " 'cans': 410,\n",
       " 'capability': 411,\n",
       " 'capitalism': 412,\n",
       " 'captain': 413,\n",
       " 'captains': 414,\n",
       " 'caputo': 415,\n",
       " 'car': 416,\n",
       " 'card': 417,\n",
       " 'cardinal': 418,\n",
       " 'cards': 419,\n",
       " 'care': 420,\n",
       " 'cared': 421,\n",
       " 'carefully': 422,\n",
       " 'caretaker': 423,\n",
       " 'carl': 424,\n",
       " 'carly': 425,\n",
       " 'carpenter': 426,\n",
       " 'carried': 427,\n",
       " 'cars': 428,\n",
       " 'case': 429,\n",
       " 'cases': 430,\n",
       " 'cash': 431,\n",
       " 'cast': 432,\n",
       " 'castle': 433,\n",
       " 'cat': 434,\n",
       " 'catch': 435,\n",
       " 'catherine': 436,\n",
       " 'cats': 437,\n",
       " 'caught': 438,\n",
       " 'cause': 439,\n",
       " 'causes': 440,\n",
       " 'caveman': 441,\n",
       " 'celebrate': 442,\n",
       " 'cell': 443,\n",
       " 'cellphone': 444,\n",
       " 'cemetery': 445,\n",
       " 'censorship': 446,\n",
       " 'center': 447,\n",
       " 'cereal': 448,\n",
       " 'certain': 449,\n",
       " 'certainly': 450,\n",
       " 'chain': 451,\n",
       " 'challenges': 452,\n",
       " 'chance': 453,\n",
       " 'change': 454,\n",
       " 'changed': 455,\n",
       " 'character': 456,\n",
       " 'charge': 457,\n",
       " 'charity': 458,\n",
       " 'charley': 459,\n",
       " 'charlie': 460,\n",
       " 'charming': 461,\n",
       " 'chases': 462,\n",
       " 'chasing': 463,\n",
       " 'chatter': 464,\n",
       " 'cheaper': 465,\n",
       " 'check': 466,\n",
       " 'checking': 467,\n",
       " 'cheer': 468,\n",
       " 'cheers': 469,\n",
       " 'chef': 470,\n",
       " 'chemistry': 471,\n",
       " 'chief': 472,\n",
       " 'child': 473,\n",
       " 'childhood': 474,\n",
       " 'childish': 475,\n",
       " 'children': 476,\n",
       " 'chilled': 477,\n",
       " 'chip': 478,\n",
       " 'chips': 479,\n",
       " 'choice': 480,\n",
       " 'choose': 481,\n",
       " 'chose': 482,\n",
       " 'christian': 483,\n",
       " 'chuck': 484,\n",
       " 'cilly': 485,\n",
       " 'circuit': 486,\n",
       " 'circumcised': 487,\n",
       " 'citation': 488,\n",
       " 'citizen': 489,\n",
       " 'city': 490,\n",
       " 'citylike': 491,\n",
       " 'civilizations': 492,\n",
       " 'clarify': 493,\n",
       " 'class': 494,\n",
       " 'classroom': 495,\n",
       " 'clean': 496,\n",
       " 'cleaning': 497,\n",
       " 'clear': 498,\n",
       " 'clearly': 499,\n",
       " 'client': 500,\n",
       " 'climate': 501,\n",
       " 'clinical': 502,\n",
       " 'clip': 503,\n",
       " 'cloak': 504,\n",
       " 'cloaking': 505,\n",
       " 'clock': 506,\n",
       " 'close': 507,\n",
       " 'closely': 508,\n",
       " 'closer': 509,\n",
       " 'clothes': 510,\n",
       " 'clubbing': 511,\n",
       " 'clutch': 512,\n",
       " 'coal': 513,\n",
       " 'code': 514,\n",
       " 'coffee': 515,\n",
       " 'coliseum': 516,\n",
       " 'collateral': 517,\n",
       " 'colonel': 518,\n",
       " 'color': 519,\n",
       " 'colored': 520,\n",
       " 'column': 521,\n",
       " 'combat': 522,\n",
       " 'combined': 523,\n",
       " 'come': 524,\n",
       " 'comes': 525,\n",
       " 'coming': 526,\n",
       " 'commercialized': 527,\n",
       " 'committed': 528,\n",
       " 'community': 529,\n",
       " 'company': 530,\n",
       " 'compete': 531,\n",
       " 'complained': 532,\n",
       " 'complete': 533,\n",
       " 'completed': 534,\n",
       " 'complex': 535,\n",
       " 'components': 536,\n",
       " 'composed': 537,\n",
       " 'comprehend': 538,\n",
       " 'compromises': 539,\n",
       " 'computer': 540,\n",
       " 'computing': 541,\n",
       " 'concealing': 542,\n",
       " 'concept': 543,\n",
       " 'concerned': 544,\n",
       " 'conclusion': 545,\n",
       " 'condensation': 546,\n",
       " 'cone': 547,\n",
       " 'conference': 548,\n",
       " 'confused': 549,\n",
       " 'congo': 550,\n",
       " 'congratulations': 551,\n",
       " 'connect': 552,\n",
       " 'connection': 553,\n",
       " 'connections': 554,\n",
       " 'conscience': 555,\n",
       " 'consider': 556,\n",
       " 'considered': 557,\n",
       " 'construct': 558,\n",
       " 'constructivist': 559,\n",
       " 'consult': 560,\n",
       " 'contact': 561,\n",
       " 'continents': 562,\n",
       " 'continue': 563,\n",
       " 'contract': 564,\n",
       " 'contradicted': 565,\n",
       " 'control': 566,\n",
       " 'convention': 567,\n",
       " 'conversation': 568,\n",
       " 'cook': 569,\n",
       " 'cooking': 570,\n",
       " 'cookout': 571,\n",
       " 'cool': 572,\n",
       " 'cope': 573,\n",
       " 'copy': 574,\n",
       " 'corporation': 575,\n",
       " 'corporations': 576,\n",
       " 'correct': 577,\n",
       " 'corridor': 578,\n",
       " 'cos': 579,\n",
       " 'cost': 580,\n",
       " 'costbenefit': 581,\n",
       " 'could': 582,\n",
       " 'council': 583,\n",
       " 'counsel': 584,\n",
       " 'counter': 585,\n",
       " 'counters': 586,\n",
       " 'countries': 587,\n",
       " 'country': 588,\n",
       " 'county': 589,\n",
       " 'couple': 590,\n",
       " 'couples': 591,\n",
       " 'courage': 592,\n",
       " 'course': 593,\n",
       " 'coworker': 594,\n",
       " 'crack': 595,\n",
       " 'crammed': 596,\n",
       " 'crap': 597,\n",
       " 'crazy': 598,\n",
       " 'create': 599,\n",
       " 'creating': 600,\n",
       " 'creation': 601,\n",
       " 'credentials': 602,\n",
       " 'credit': 603,\n",
       " 'creeping': 604,\n",
       " 'crew': 605,\n",
       " 'crib': 606,\n",
       " 'crime': 607,\n",
       " 'crochet': 608,\n",
       " 'cross': 609,\n",
       " 'crossreferenced': 610,\n",
       " 'crown': 611,\n",
       " 'crows': 612,\n",
       " 'crush': 613,\n",
       " 'cry': 614,\n",
       " 'culprit': 615,\n",
       " 'culture': 616,\n",
       " 'curious': 617,\n",
       " 'current': 618,\n",
       " 'curse': 619,\n",
       " 'curtain': 620,\n",
       " 'cusses': 621,\n",
       " 'customers': 622,\n",
       " 'cut': 623,\n",
       " 'cute': 624,\n",
       " 'cuts': 625,\n",
       " 'cyanobacteria': 626,\n",
       " 'cycle': 627,\n",
       " 'd': 628,\n",
       " 'dad': 629,\n",
       " 'dagger': 630,\n",
       " 'dale': 631,\n",
       " 'damn': 632,\n",
       " 'dan': 633,\n",
       " 'danger': 634,\n",
       " 'dangerous': 635,\n",
       " 'danny': 636,\n",
       " 'dare': 637,\n",
       " 'dark': 638,\n",
       " 'darn': 639,\n",
       " 'data': 640,\n",
       " 'database': 641,\n",
       " 'date': 642,\n",
       " 'daughter': 643,\n",
       " 'day': 644,\n",
       " 'days': 645,\n",
       " 'dc': 646,\n",
       " 'dead': 647,\n",
       " 'deal': 648,\n",
       " 'deals': 649,\n",
       " 'dealt': 650,\n",
       " 'dear': 651,\n",
       " 'death': 652,\n",
       " 'debacle': 653,\n",
       " 'debt': 654,\n",
       " 'decade': 655,\n",
       " 'decent': 656,\n",
       " 'decided': 657,\n",
       " 'deciding': 658,\n",
       " 'decisions': 659,\n",
       " 'decisive': 660,\n",
       " 'declining': 661,\n",
       " 'deemed': 662,\n",
       " 'deep': 663,\n",
       " 'deformations': 664,\n",
       " 'delegates': 665,\n",
       " 'delia': 666,\n",
       " 'delicious': 667,\n",
       " 'demand': 668,\n",
       " 'demands': 669,\n",
       " 'democracy': 670,\n",
       " 'democratic': 671,\n",
       " 'demographers': 672,\n",
       " 'demon': 673,\n",
       " 'denying': 674,\n",
       " 'department': 675,\n",
       " 'depend': 676,\n",
       " 'deposit': 677,\n",
       " 'depth': 678,\n",
       " 'depths': 679,\n",
       " 'deru': 680,\n",
       " 'design': 681,\n",
       " 'designers': 682,\n",
       " 'desk': 683,\n",
       " 'desperate': 684,\n",
       " 'despite': 685,\n",
       " 'destiny': 686,\n",
       " 'destroyed': 687,\n",
       " 'destruction': 688,\n",
       " 'detailed': 689,\n",
       " 'detective': 690,\n",
       " 'detonation': 691,\n",
       " 'developing': 692,\n",
       " 'device': 693,\n",
       " 'devices': 694,\n",
       " 'diagnosis': 695,\n",
       " 'diamond': 696,\n",
       " 'diary': 697,\n",
       " 'did': 698,\n",
       " 'didactic': 699,\n",
       " 'die': 700,\n",
       " 'died': 701,\n",
       " 'difference': 702,\n",
       " 'different': 703,\n",
       " 'digging': 704,\n",
       " 'digimon': 705,\n",
       " 'dimwit': 706,\n",
       " 'directly': 707,\n",
       " 'director': 708,\n",
       " 'disappear': 709,\n",
       " 'disappearing': 710,\n",
       " 'disappointed': 711,\n",
       " 'disaster': 712,\n",
       " 'disasters': 713,\n",
       " 'disclose': 714,\n",
       " 'disclosure': 715,\n",
       " 'disconnected': 716,\n",
       " 'discourse': 717,\n",
       " 'discovered': 718,\n",
       " 'discussion': 719,\n",
       " 'discussions': 720,\n",
       " 'disease': 721,\n",
       " 'diseases': 722,\n",
       " 'disgusting': 723,\n",
       " 'disinfect': 724,\n",
       " 'dislike': 725,\n",
       " 'dispel': 726,\n",
       " 'dissolve': 727,\n",
       " 'distance': 728,\n",
       " 'distribute': 729,\n",
       " 'disturb': 730,\n",
       " 'disturbances': 731,\n",
       " 'dive': 732,\n",
       " 'divine': 733,\n",
       " 'division': 734,\n",
       " 'divorce': 735,\n",
       " 'divorced': 736,\n",
       " 'dj': 737,\n",
       " 'dna': 738,\n",
       " 'do': 739,\n",
       " 'doctor': 740,\n",
       " 'does': 741,\n",
       " 'dog': 742,\n",
       " 'doin': 743,\n",
       " 'doing': 744,\n",
       " 'doll': 745,\n",
       " 'done': 746,\n",
       " 'door': 747,\n",
       " 'doors': 748,\n",
       " 'dorks': 749,\n",
       " 'doubt': 750,\n",
       " 'dougami': 751,\n",
       " 'dove': 752,\n",
       " 'down': 753,\n",
       " 'downstairs': 754,\n",
       " 'dr': 755,\n",
       " 'dracula': 756,\n",
       " 'draw': 757,\n",
       " 'dream': 758,\n",
       " 'dreaming': 759,\n",
       " 'dreams': 760,\n",
       " 'dress': 761,\n",
       " 'drew': 762,\n",
       " 'drill': 763,\n",
       " 'drink': 764,\n",
       " 'drive': 765,\n",
       " 'driver': 766,\n",
       " 'drop': 767,\n",
       " 'drug': 768,\n",
       " 'due': 769,\n",
       " 'duel': 770,\n",
       " 'dug': 771,\n",
       " 'dull': 772,\n",
       " 'dunham': 773,\n",
       " 'during': 774,\n",
       " 'dynamics': 775,\n",
       " 'e': 776,\n",
       " 'each': 777,\n",
       " 'ear': 778,\n",
       " 'earl': 779,\n",
       " 'early': 780,\n",
       " 'earn': 781,\n",
       " 'earth': 782,\n",
       " 'easily': 783,\n",
       " 'east': 784,\n",
       " 'easy': 785,\n",
       " 'eat': 786,\n",
       " 'eating': 787,\n",
       " 'economically': 788,\n",
       " 'ecstasy': 789,\n",
       " 'eddie': 790,\n",
       " 'eden': 791,\n",
       " 'edge': 792,\n",
       " 'edit': 793,\n",
       " 'edited': 794,\n",
       " 'editor': 795,\n",
       " 'editorinchief': 796,\n",
       " 'education': 797,\n",
       " 'edward': 798,\n",
       " 'effect': 799,\n",
       " 'effects': 800,\n",
       " 'egyptians': 801,\n",
       " 'eh': 802,\n",
       " 'eight': 803,\n",
       " 'either': 804,\n",
       " 'elaine': 805,\n",
       " 'eleanor': 806,\n",
       " 'election': 807,\n",
       " 'electricity': 808,\n",
       " 'elephant': 809,\n",
       " 'elliot': 810,\n",
       " 'else': 811,\n",
       " 'em': 812,\n",
       " 'embarrassing': 813,\n",
       " 'embellishments': 814,\n",
       " 'embodied': 815,\n",
       " 'emerald': 816,\n",
       " 'emergency': 817,\n",
       " 'emma': 818,\n",
       " 'emphasize': 819,\n",
       " 'employee': 820,\n",
       " 'employees': 821,\n",
       " 'employers': 822,\n",
       " 'encountered': 823,\n",
       " 'encouraged': 824,\n",
       " 'encrypted': 825,\n",
       " 'end': 826,\n",
       " 'endure': 827,\n",
       " 'enemies': 828,\n",
       " 'enemy': 829,\n",
       " 'energetic': 830,\n",
       " 'energy': 831,\n",
       " 'energyconsuming': 832,\n",
       " 'engage': 833,\n",
       " 'engagement': 834,\n",
       " 'engine': 835,\n",
       " 'engineering': 836,\n",
       " 'engineers': 837,\n",
       " 'engines': 838,\n",
       " 'enjoyed': 839,\n",
       " 'enlightened': 840,\n",
       " 'enormously': 841,\n",
       " 'enough': 842,\n",
       " 'entire': 843,\n",
       " 'entitle': 844,\n",
       " 'envious': 845,\n",
       " 'environment': 846,\n",
       " 'equation': 847,\n",
       " 'equestria': 848,\n",
       " 'equipment': 849,\n",
       " 'equivalent': 850,\n",
       " 'erica': 851,\n",
       " 'erikosan': 852,\n",
       " 'escaped': 853,\n",
       " 'essentially': 854,\n",
       " 'establish': 855,\n",
       " 'estate': 856,\n",
       " 'eternity': 857,\n",
       " 'ethical': 858,\n",
       " 'ethics': 859,\n",
       " 'eugene': 860,\n",
       " 'europe': 861,\n",
       " 'evading': 862,\n",
       " 'even': 863,\n",
       " 'event': 864,\n",
       " 'ever': 865,\n",
       " 'everaftering': 866,\n",
       " 'every': 867,\n",
       " 'everybody': 868,\n",
       " 'everyone': 869,\n",
       " 'everything': 870,\n",
       " 'everywhere': 871,\n",
       " 'evidence': 872,\n",
       " 'evidently': 873,\n",
       " 'evie': 874,\n",
       " 'evil': 875,\n",
       " 'evolution': 876,\n",
       " 'evolve': 877,\n",
       " 'evolved': 878,\n",
       " 'ew': 879,\n",
       " 'exact': 880,\n",
       " 'exactly': 881,\n",
       " 'exaggerated': 882,\n",
       " 'example': 883,\n",
       " 'except': 884,\n",
       " 'exceptional': 885,\n",
       " 'exchange': 886,\n",
       " 'exciting': 887,\n",
       " 'excursion': 888,\n",
       " 'excuse': 889,\n",
       " 'excuses': 890,\n",
       " 'execution': 891,\n",
       " 'exercise': 892,\n",
       " 'exert': 893,\n",
       " 'exhausted': 894,\n",
       " 'existed': 895,\n",
       " 'exists': 896,\n",
       " 'expand': 897,\n",
       " 'expect': 898,\n",
       " 'expectation': 899,\n",
       " 'expected': 900,\n",
       " 'expensive': 901,\n",
       " 'experience': 902,\n",
       " 'experiences': 903,\n",
       " 'experiment': 904,\n",
       " 'explanation': 905,\n",
       " 'exploded': 906,\n",
       " 'explosives': 907,\n",
       " 'extent': 908,\n",
       " 'exterminate': 909,\n",
       " 'extremely': 910,\n",
       " 'eye': 911,\n",
       " 'eyes': 912,\n",
       " 'f': 913,\n",
       " 'face': 914,\n",
       " 'faced': 915,\n",
       " 'facing': 916,\n",
       " 'fact': 917,\n",
       " 'faction': 918,\n",
       " 'factors': 919,\n",
       " 'factual': 920,\n",
       " 'failed': 921,\n",
       " 'failure': 922,\n",
       " 'failures': 923,\n",
       " 'fake': 924,\n",
       " 'fall': 925,\n",
       " 'fallen': 926,\n",
       " 'familia': 927,\n",
       " 'families': 928,\n",
       " 'family': 929,\n",
       " 'famous': 930,\n",
       " 'fan': 931,\n",
       " 'fancy': 932,\n",
       " 'fantastic': 933,\n",
       " 'far': 934,\n",
       " 'farmers': 935,\n",
       " 'fascinating': 936,\n",
       " 'fast': 937,\n",
       " 'faster': 938,\n",
       " 'father': 939,\n",
       " 'fathers': 940,\n",
       " 'fault': 941,\n",
       " 'favor': 942,\n",
       " 'favorite': 943,\n",
       " 'fbi': 944,\n",
       " 'fda': 945,\n",
       " 'fear': 946,\n",
       " 'feedbacks': 947,\n",
       " 'feel': 948,\n",
       " 'feeling': 949,\n",
       " 'feelings': 950,\n",
       " 'fell': 951,\n",
       " 'fella': 952,\n",
       " 'fellow': 953,\n",
       " 'felt': 954,\n",
       " 'few': 955,\n",
       " 'field': 956,\n",
       " 'fields': 957,\n",
       " 'fight': 958,\n",
       " 'figure': 959,\n",
       " 'figured': 960,\n",
       " 'file': 961,\n",
       " 'files': 962,\n",
       " 'fin': 963,\n",
       " 'final': 964,\n",
       " 'finally': 965,\n",
       " 'finals': 966,\n",
       " 'find': 967,\n",
       " 'finding': 968,\n",
       " 'fine': 969,\n",
       " 'finish': 970,\n",
       " 'finishing': 971,\n",
       " 'fire': 972,\n",
       " 'firefly': 973,\n",
       " 'first': 974,\n",
       " 'fish': 975,\n",
       " 'fishery': 976,\n",
       " 'fit': 977,\n",
       " 'fitforservice': 978,\n",
       " 'five': 979,\n",
       " 'fixes': 980,\n",
       " 'flamethrower': 981,\n",
       " 'flatter': 982,\n",
       " 'fleeting': 983,\n",
       " 'flight': 984,\n",
       " 'floating': 985,\n",
       " 'flock': 986,\n",
       " 'floor': 987,\n",
       " 'florence': 988,\n",
       " 'flow': 989,\n",
       " 'flowerpot': 990,\n",
       " 'flowers': 991,\n",
       " 'flunk': 992,\n",
       " 'fly': 993,\n",
       " 'flying': 994,\n",
       " 'folk': 995,\n",
       " 'follow': 996,\n",
       " 'following': 997,\n",
       " 'food': 998,\n",
       " 'fool': 999,\n",
       " 'for': 1000,\n",
       " 'forced': 1001,\n",
       " 'forces': 1002,\n",
       " 'forensics': 1003,\n",
       " ...}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from docopt import docopt\n",
    "from itertools import chain\n",
    "import json\n",
    "import torch\n",
    "from typing import List\n",
    "from utils import read_corpus, pad_sents\n",
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "class VocabEntry(object):\n",
    "    \"\"\" Vocabulary Entry, i.e. structure containing either\n",
    "    src or tgt language terms.\n",
    "    \"\"\"\n",
    "    def __init__(self, word2id=None):\n",
    "        \"\"\" Init VocabEntry Instance.\n",
    "        @param word2id (dict): dictionary mapping words 2 indices\n",
    "        \"\"\"\n",
    "        if word2id:\n",
    "            self.word2id = word2id\n",
    "        else:\n",
    "            self.word2id = dict()\n",
    "            self.word2id['<pad>'] = 0   # Pad Token\n",
    "            self.word2id['<s>'] = 1 # Start Token\n",
    "            self.word2id['</s>'] = 2    # End Token\n",
    "            self.word2id['<unk>'] = 3   # Unknown Token\n",
    "        self.unk_id = self.word2id['<unk>']\n",
    "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
    "\n",
    "    def __getitem__(self, word):\n",
    "        \"\"\" Retrieve word's index. Return the index for the unk\n",
    "        token if the word is out of vocabulary.\n",
    "        @param word (str): word to look up.\n",
    "        @returns index (int): index of word \n",
    "        \"\"\"\n",
    "        return self.word2id.get(word, self.unk_id)\n",
    "\n",
    "    def __contains__(self, word):\n",
    "        \"\"\" Check if word is captured by VocabEntry.\n",
    "        @param word (str): word to look up\n",
    "        @returns contains (bool): whether word is contained    \n",
    "        \"\"\"\n",
    "        return word in self.word2id\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        \"\"\" Raise error, if one tries to edit the VocabEntry.\n",
    "        \"\"\"\n",
    "        raise ValueError('vocabulary is readonly')\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Compute number of words in VocabEntry.\n",
    "        @returns len (int): number of words in VocabEntry\n",
    "        \"\"\"\n",
    "        return len(self.word2id)\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\" Representation of VocabEntry to be used\n",
    "        when printing the object.\n",
    "        \"\"\"\n",
    "        return 'Vocabulary[size=%d]' % len(self)\n",
    "\n",
    "    def id2word(self, wid):\n",
    "        \"\"\" Return mapping of index to word.\n",
    "        @param wid (int): word index\n",
    "        @returns word (str): word corresponding to index\n",
    "        \"\"\"\n",
    "        return self.id2word[wid]\n",
    "\n",
    "    def add(self, word):\n",
    "        \"\"\" Add word to VocabEntry, if it is previously unseen.\n",
    "        @param word (str): word to add to VocabEntry\n",
    "        @return index (int): index that the word has been assigned\n",
    "        \"\"\"\n",
    "        if word not in self:\n",
    "            wid = self.word2id[word] = len(self)\n",
    "            self.id2word[wid] = word\n",
    "            return wid\n",
    "        else:\n",
    "            return self[word]\n",
    "\n",
    "    def words2indices(self, sents):\n",
    "        \"\"\" Convert list of words or list of sentences of words\n",
    "        into list or list of list of indices.\n",
    "        @param sents (list[str] or list[list[str]]): sentence(s) in words\n",
    "        @return word_ids (list[int] or list[list[int]]): sentence(s) in indices\n",
    "        \"\"\"\n",
    "        if type(sents[0]) == list:\n",
    "            return [[self[w] for w in s] for s in sents]\n",
    "        else:\n",
    "            return [self[w] for w in sents]\n",
    "\n",
    "    def indices2words(self, word_ids):\n",
    "        \"\"\" Convert list of indices into words.\n",
    "        @param word_ids (list[int]): list of word ids\n",
    "        @return sents (list[str]): list of words\n",
    "        \"\"\"\n",
    "        return [self.id2word[w_id] for w_id in word_ids]\n",
    "\n",
    "    def to_input_tensor(self, sents: List[List[str]], device: torch.device) -> torch.Tensor:\n",
    "        \"\"\" Convert list of sentences (words) into tensor with necessary padding for \n",
    "        shorter sentences.\n",
    "\n",
    "        @param sents (List[List[str]]): list of sentences (words)\n",
    "        @param device: device on which to load the tesnor, i.e. CPU or GPU\n",
    "\n",
    "        @returns sents_var: tensor of (max_sentence_length, batch_size)\n",
    "        \"\"\"\n",
    "        word_ids = self.words2indices(sents)\n",
    "        sents_t = pad_sents(word_ids, self['<pad>'])\n",
    "        sents_var = torch.tensor(sents_t, dtype=torch.long, device=device)\n",
    "        return torch.t(sents_var)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_corpus(corpus, size, freq_cutoff=2):\n",
    "        \"\"\" Given a corpus construct a Vocab Entry.\n",
    "        @param corpus (list[str]): corpus of text produced by read_corpus function\n",
    "        @param size (int): # of words in vocabulary\n",
    "        @param freq_cutoff (int): if word occurs n < freq_cutoff times, drop the word\n",
    "        @returns vocab_entry (VocabEntry): VocabEntry instance produced from provided corpus\n",
    "        \"\"\"\n",
    "        vocab_entry = VocabEntry()\n",
    "        word_freq = Counter(chain(*corpus))\n",
    "        valid_words = [w for w, v in word_freq.items() if v >= freq_cutoff]\n",
    "        print('number of word types: {}, number of word types w/ frequency >= {}: {}'\n",
    "              .format(len(word_freq), freq_cutoff, len(valid_words)))\n",
    "        top_k_words = sorted(valid_words, key=lambda w: word_freq[w], reverse=True)[:size]\n",
    "        for word in top_k_words:\n",
    "            vocab_entry.add(word)\n",
    "        return vocab_entry\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_subword_list(subword_list):\n",
    "        vocab_entry = VocabEntry()\n",
    "        for subword in subword_list:\n",
    "            vocab_entry.add(subword)\n",
    "        return vocab_entry\n",
    "\n",
    "\n",
    "class Vocab(object):\n",
    "    \"\"\" Vocab encapsulating src and target langauges.\n",
    "    \"\"\"\n",
    "    def __init__(self, src_vocab: VocabEntry, tgt_vocab: VocabEntry):\n",
    "        \"\"\" Init Vocab.\n",
    "        @param src_vocab (VocabEntry): VocabEntry for source language\n",
    "        @param tgt_vocab (VocabEntry): VocabEntry for target language\n",
    "        \"\"\"\n",
    "        self.src = src_vocab\n",
    "        self.tgt = tgt_vocab\n",
    "\n",
    "    @staticmethod\n",
    "    def build(src_sents, tgt_sents) -> 'Vocab':\n",
    "        \"\"\" Build Vocabulary.\n",
    "        @param src_sents (list[str]): Source subwords provided by SentencePiece\n",
    "        @param tgt_sents (list[str]): Target subwords provided by SentencePiece\n",
    "        \"\"\"\n",
    "        # assert len(src_sents) == len(tgt_sents)\n",
    "\n",
    "        print('initialize source vocabulary ..')\n",
    "        # src = VocabEntry.from_corpus(src_sents, vocab_size, freq_cutoff)\n",
    "        src = VocabEntry.from_subword_list(src_sents)\n",
    "\n",
    "        print('initialize target vocabulary ..')\n",
    "        # tgt = VocabEntry.from_corpus(tgt_sents, vocab_size, freq_cutoff)\n",
    "        tgt = VocabEntry.from_subword_list(tgt_sents)\n",
    "\n",
    "        return Vocab(src, tgt)\n",
    "\n",
    "    def save(self, file_path):\n",
    "        \"\"\" Save Vocab to file as JSON dump.\n",
    "        @param file_path (str): file path to vocab file\n",
    "        \"\"\"\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(dict(src_word2id=self.src.word2id, tgt_word2id=self.tgt.word2id), f, indent=2)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(file_path):\n",
    "        \"\"\" Load vocabulary from JSON dump.\n",
    "        @param file_path (str): file path to vocab file\n",
    "        @returns Vocab object loaded from JSON dump\n",
    "        \"\"\"\n",
    "        entry = json.load(open(file_path, 'r'))\n",
    "        src_word2id = entry['src_word2id']\n",
    "        tgt_word2id = entry['tgt_word2id']\n",
    "\n",
    "        return Vocab(VocabEntry(src_word2id), VocabEntry(tgt_word2id))\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\" Representation of Vocab to be used\n",
    "        when printing the object.\n",
    "        \"\"\"\n",
    "        return 'Vocab(source %d words, target %d words)' % (len(self.src), len(self.tgt))\n",
    "\n",
    "def get_vocab_list(file_path, source, vocab_size):\n",
    "    \"\"\" Use SentencePiece to tokenize and acquire list of unique subwords.\n",
    "    @param file_path (str): file path to corpus\n",
    "    @param source (str): tgt or src\n",
    "    @param vocab_size: desired vocabulary size\n",
    "    \"\"\" \n",
    "    spm.SentencePieceTrainer.Train(input=file_path, model_prefix=source, vocab_size=vocab_size)     # train the spm model\n",
    "    sp = spm.SentencePieceProcessor()   # create an instance; this saves .model and .vocab files \n",
    "    sp.Load('{}.model'.format(source))  # loads tgt.model or src.model\n",
    "    sp_list = [sp.IdToPiece(piece_id) for piece_id in range(sp.GetPieceSize())] # this is the list of subwords\n",
    "    return sp_list \n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = docopt(__doc__)\n",
    "\n",
    "    print('read in source sentences: %s' % args['--train-src'])\n",
    "    print('read in target sentences: %s' % args['--train-tgt'])\n",
    "\n",
    "    src_sents = get_vocab_list(args['--train-src'], source='src', vocab_size=32000)          # EDIT: NEW VOCAB SIZE\n",
    "    tgt_sents = get_vocab_list(args['--train-tgt'], source='tgt', vocab_size=8000)\n",
    "    vocab = Vocab.build(src_sents, tgt_sents)\n",
    "    print('generated vocabulary, source %d words, target %d words' % (len(src_sents), len(tgt_sents)))\n",
    "\n",
    "    # src_sents = read_corpus(args['--train-src'], source='src')\n",
    "    # tgt_sents = read_corpus(args['--train-tgt'], source='tgt')\n",
    "\n",
    "    # vocab = Vocab.build(src_sents, tgt_sents, int(args['--size']), int(args['--freq-cutoff']))\n",
    "    # print('generated vocabulary, source %d words, target %d words' % (len(vocab.src), len(vocab.tgt)))\n",
    "\n",
    "    vocab.save(args['VOCAB_FILE'])\n",
    "    print('vocabulary saved to %s' % args['VOCAB_FILE'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47551da3-4492-45d9-8483-401627ba43a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
